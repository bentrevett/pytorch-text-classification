{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regional-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datasets\n",
    "import functools\n",
    "import mininlp\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-paper",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noble-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "imdb = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dangerous-quick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "massive-stereo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "written-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset('imdb', split=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intimate-olive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apart-patent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "southern-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_split(train_data, valid_ratio=0.2, shuffle=True):\n",
    "    data = train_data.train_test_split(test_size=valid_ratio, shuffle=shuffle)\n",
    "    train_data = data['train']\n",
    "    valid_data = data['test']\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "political-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-920dca5bb59550b9.arrow and /home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-65748ac44d181710.arrow\n"
     ]
    }
   ],
   "source": [
    "valid_ratio = 0.2\n",
    "shuffle = True\n",
    "\n",
    "train_data, valid_data = get_train_valid_split(train_data, valid_ratio, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expected-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 5000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(valid_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-elite",
   "metadata": {},
   "source": [
    "## Initializing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "enclosed-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_fn = lambda x : x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "welsh-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mininlp.tokenizer.Tokenizer(tokenize_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "latest-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = 'Hello world! How is everyone doing today?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "described-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world!', 'How', 'is', 'everyone', 'doing', 'today?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chronic-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_tokenize(s: str, nlp: spacy.lang):\n",
    "    return [t.text for t in nlp.tokenizer(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "express-source",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!', 'How', 'is', 'everyone', 'doing', 'today', '?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenize(example_string, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intimate-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "_spacy_tokenize = functools.partial(spacy_tokenize, nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exceptional-repeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!', 'How', 'is', 'everyone', 'doing', 'today', '?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spacy_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "convenient-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mininlp.tokenizer.Tokenizer(_spacy_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "banned-serve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!', 'How', 'is', 'everyone', 'doing', 'today', '?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-customs",
   "metadata": {},
   "source": [
    "## Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "trained-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'text'\n",
    "\n",
    "counter = mininlp.vocab.build_vocab_counter(train_data, field, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "listed-express",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 232322),\n",
       " (',', 220773),\n",
       " ('.', 189909),\n",
       " ('a', 125392),\n",
       " ('and', 125260),\n",
       " ('of', 115263),\n",
       " ('to', 107115),\n",
       " ('is', 87381),\n",
       " ('in', 70335),\n",
       " ('I', 61975)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "renewable-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "max_size = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sunset-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = mininlp.vocab.Vocab(counter, min_freq, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "worth-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28386"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "necessary-voltage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7594"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "heated-cream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11977"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "instrumental-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos(11977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "average-ghost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi('Cthulhu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "broke-blair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "composite-pearl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'How', 'is', 'everyone', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "example_string = 'Hello world! How is everyone doing today?'\n",
    "\n",
    "example_tokens = tokenizer.tokenize(example_string)\n",
    "\n",
    "print(example_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "egyptian-acceptance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7594, 223, 42, 568, 9, 353, 428, 572, 58]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi(example_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "collective-species",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!', 'How', 'is', 'everyone', 'doing', 'today', '?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos(vocab.stoi(example_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "frank-husband",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'best', 'friend', 'is', 'named', '<unk>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string = 'My best friend is named Cthulhu'\n",
    "\n",
    "example_tokens = tokenizer.tokenize(example_string)\n",
    "\n",
    "vocab.itos(vocab.stoi(example_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-coaching",
   "metadata": {},
   "source": [
    "## Creating the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "legitimate-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transforms = mininlp.transforms.sequential_transforms(tokenizer.tokenize,\n",
    "                                                           vocab.stoi,\n",
    "                                                           mininlp.transforms.to_longtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "declared-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transforms = mininlp.transforms.sequential_transforms(mininlp.transforms.to_longtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "convertible-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mininlp.dataset.TextClassificationDataset(train_data, text_transforms, label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "loving-arthur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7473,  2616,    11,   277,     8,    36,    43,   348,   836,    18,\n",
       "           707,    31,    47,  5534,    15,  4419,    31,   264,    47,  2955,\n",
       "            18,  4015,     2,    22,     9,   176,     6,    67,    32,    82,\n",
       "           343,   323,     6,    59,     7,     2,   131,    69,   648,    36,\n",
       "           144,  2160,    20,    14,   993,    14,    26,     2,    75,     9,\n",
       "         16574,    18,   116,    75,     7,   111,     2,   223,    69,    36,\n",
       "             6,   112,  1674,    29,  1024,     9,    62,     0,    18,  2536,\n",
       "             2,   150,     2,   925,    19,    28,     5,   925,    26,     2,\n",
       "            75, 12995,    10,    43,   851,    40,     2,   233, 22119,    18,\n",
       "           425,    15,  9308,   715,   293,  4716,    63,    36,    14,  2067,\n",
       "          4369,    14,    10,  1338,     7,   111,     2,  1174,     9,   644,\n",
       "             8,    36,   907,    17,   223,   790,     4]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sufficient-internet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Soylent Green I found to be an excellent movie.<br /><br />If you like Logan\\'s Run you\\'ll like this.<br /><br />Yes the movie is old and there are no special effects and some of the acting can somewhat be best described as \"cheesy\" but the story is excellent.<br /><br />The story of how the world can be and its impact on society is very poignant.<br /><br />At the end the mystery wasn\\'t a mystery but the story unfolded in an easy at the right pace.<br /><br />It\\'s nearest modern day equivalent would be \"Dark Angel\" in terms of how the US is shown to be third-world country.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "possible-clearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi('Soylent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "third-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = mininlp.dataset.TextClassificationDataset(valid_data, text_transforms, label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "prescribed-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = mininlp.dataset.TextClassificationDataset(test_data, text_transforms, label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mobile-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "pad_idx = vocab.stoi(vocab.pad_token)\n",
    "\n",
    "print(pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "regular-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = mininlp.collator.TextClassificationCollator(pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "boolean-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "white-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           collate_fn=collator.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "duplicate-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           collate_fn=collator.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "committed-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, \n",
    "                                          collate_fn=collator.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-norwegian",
   "metadata": {},
   "source": [
    "## Creating the NBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "amateur-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBOW(nn.Module):\n",
    "    def __init__(self, input_dim: int, emb_dim: int, output_dim: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        # text = [seq len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # embedded = [seq len, batch size, emb dim]\n",
    "        \n",
    "        pooled = embedded.mean(0)\n",
    "        \n",
    "        # pooled = [batch size, emb dim]\n",
    "        \n",
    "        prediction = self.fc(pooled)\n",
    "        \n",
    "        # prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sitting-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "emb_dim = 100\n",
    "output_dim = 2\n",
    "\n",
    "model = NBOW(input_dim, emb_dim, output_dim, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "initial-license",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,838,802 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {mininlp.utils.count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "tight-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "identified-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "alpine-peninsula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "second-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "engaging-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for text, labels in data_loader:\n",
    "        \n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(text)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = mininlp.utils.calculate_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "thrown-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for text, labels in data_loader:\n",
    "\n",
    "            text = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = mininlp.utils.calculate_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "corresponding-semester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e10e0d80f94cedbefc924961c9402f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  Train Loss: 0.691 | Train Acc: 53.32%\n",
      "  Valid Loss: 0.686 | Valid Acc: 62.64%\n",
      "Epoch:  1\n",
      "  Train Loss: 0.676 | Train Acc: 68.02%\n",
      "  Valid Loss: 0.668 | Valid Acc: 70.93%\n",
      "Epoch:  2\n",
      "  Train Loss: 0.648 | Train Acc: 73.11%\n",
      "  Valid Loss: 0.637 | Valid Acc: 73.11%\n",
      "Epoch:  3\n",
      "  Train Loss: 0.609 | Train Acc: 76.55%\n",
      "  Valid Loss: 0.599 | Valid Acc: 76.36%\n",
      "Epoch:  4\n",
      "  Train Loss: 0.566 | Train Acc: 79.25%\n",
      "  Valid Loss: 0.562 | Valid Acc: 78.44%\n",
      "Epoch:  5\n",
      "  Train Loss: 0.522 | Train Acc: 81.53%\n",
      "  Valid Loss: 0.527 | Valid Acc: 80.63%\n",
      "Epoch:  6\n",
      "  Train Loss: 0.483 | Train Acc: 83.35%\n",
      "  Valid Loss: 0.495 | Valid Acc: 82.26%\n",
      "Epoch:  7\n",
      "  Train Loss: 0.445 | Train Acc: 85.44%\n",
      "  Valid Loss: 0.467 | Valid Acc: 83.05%\n",
      "Epoch:  8\n",
      "  Train Loss: 0.414 | Train Acc: 86.93%\n",
      "  Valid Loss: 0.443 | Valid Acc: 83.94%\n",
      "Epoch:  9\n",
      "  Train Loss: 0.385 | Train Acc: 87.86%\n",
      "  Valid Loss: 0.422 | Valid Acc: 84.75%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm.notebook.tqdm(range(n_epochs)):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'nbow-model.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch:2}')\n",
    "    print(f'  Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'  Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "radical-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.368 | Test Acc: 88.40%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('nbow-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fatty-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, text_transforms, model, device):\n",
    "    model.eval()\n",
    "    tensor = text_transforms(sentence).unsqueeze(-1).to(device)\n",
    "    prediction = model(tensor)\n",
    "    probabilities = nn.functional.softmax(prediction, dim=-1)\n",
    "    pos_probability = probabilities.squeeze(0)[-1].item()\n",
    "    return pos_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "floral-bennett",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0972864806244615e-07"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'the absolute worst movie of all time.'\n",
    "\n",
    "predict(sentence, text_transforms, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "recognized-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'one of the greatest films i have ever seen in my life.'\n",
    "\n",
    "predict(sentence, text_transforms, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "expanded-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8011810779571533"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"i thought it was going to be one of the greatest films i have ever seen in my life, \\\n",
    "but it was actually the absolute worst movie of all time.\"\n",
    "\n",
    "predict(sentence, text_transforms, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "molecular-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8011810779571533"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"i thought it was going to be the absolute worst movie of all time, \\\n",
    "but it was actually one of the greatest films i have ever seen in my life.\"\n",
    "\n",
    "predict(sentence, text_transforms, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
