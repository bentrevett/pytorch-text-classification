{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regional-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datasets\n",
    "import functools\n",
    "import mininlp\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-prayer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "\n",
    "_ = pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-paper",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "written-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset('imdb', split=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southern-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_split(train_data, valid_ratio=0.2, shuffle=True):\n",
    "    data = train_data.train_test_split(test_size=valid_ratio, shuffle=shuffle)\n",
    "    train_data = data['train']\n",
    "    valid_data = data['test']\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "political-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-c7753a32c7c1dfde.arrow and /home/ben/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-69d48dab60f93b72.arrow\n"
     ]
    }
   ],
   "source": [
    "valid_ratio = 0.2\n",
    "shuffle = True\n",
    "\n",
    "train_data, valid_data = get_train_valid_split(train_data, valid_ratio, shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-elite",
   "metadata": {},
   "source": [
    "## Initializing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chronic-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_tokenize(s: str, nlp: spacy.lang):\n",
    "    return [t.text for t in nlp.tokenizer(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intimate-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "_spacy_tokenize = functools.partial(spacy_tokenize, nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convenient-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mininlp.tokenizer.Tokenizer(_spacy_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-customs",
   "metadata": {},
   "source": [
    "## Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trained-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'text'\n",
    "\n",
    "counter = mininlp.vocab.build_vocab_counter(train_data, field, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sunset-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "max_size = 30_000\n",
    "\n",
    "vocab = mininlp.vocab.Vocab(counter, min_freq, max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-coaching",
   "metadata": {},
   "source": [
    "## Creating the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "legitimate-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transforms = mininlp.transforms.sequential_transforms(tokenizer.tokenize,\n",
    "                                                           vocab.stoi,\n",
    "                                                           mininlp.transforms.to_longtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "declared-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transforms = mininlp.transforms.sequential_transforms(mininlp.transforms.to_longtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "convertible-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mininlp.dataset.TextClassificationDataset(train_data, text_transforms, label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "third-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = mininlp.dataset.TextClassificationDataset(valid_data, text_transforms, label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prescribed-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = mininlp.dataset.TextClassificationDataset(test_data, text_transforms, label_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mobile-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = vocab.stoi(vocab.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "regular-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = mininlp.collator.TextClassificationCollator(pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "boolean-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "white-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           collate_fn=collator.collate,\n",
    "                                           num_workers=torch.get_num_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "duplicate-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           collate_fn=collator.collate,\n",
    "                                           num_workers=torch.get_num_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "committed-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, \n",
    "                                          collate_fn=collator.collate,\n",
    "                                          num_workers=torch.get_num_threads())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-norwegian",
   "metadata": {},
   "source": [
    "## Creating the NBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "burning-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBOW(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int, emb_dim: int, output_dim: int, pad_idx: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = pad_idx)\n",
    "        self.fc = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text: torch.LongTensor) -> torch.FloatTensor:\n",
    "        #text = [seq len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [seq len, batch size, emb dim]\n",
    "        pooled = embedded.mean(0)\n",
    "        # pooled = [batch size, emb dim]\n",
    "        prediction = self.fc(pooled)\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, labels = batch\n",
    "        predictions = self.forward(text)\n",
    "        loss = F.cross_entropy(predictions, labels)\n",
    "        acc = mininlp.utils.calculate_accuracy(predictions, labels)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        loss, acc = self.calculate_metrics(training_step_outputs)\n",
    "        print(f'Epoch: {self.current_epoch:2}')\n",
    "        print(f'  Train Loss: {loss:.3f} | Train Acc: {acc*100:.2f}%')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, labels = batch\n",
    "        predictions = self.forward(text)\n",
    "        loss = F.cross_entropy(predictions, labels)\n",
    "        acc = mininlp.utils.calculate_accuracy(predictions, labels)\n",
    "        self.log('valid_loss', loss)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        loss, acc = self.calculate_metrics(validation_step_outputs)\n",
    "        print(f'  Valid Loss: {loss:.3f} | Valid Acc: {acc*100:.2f}%')\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        text, labels = batch\n",
    "        predictions = self.forward(text)\n",
    "        loss = F.cross_entropy(predictions, labels)\n",
    "        acc = mininlp.utils.calculate_accuracy(predictions, labels)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "        \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        loss, acc = self.calculate_metrics(test_step_outputs)\n",
    "        print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters())\n",
    "    \n",
    "    def calculate_metrics(self, step_outputs):\n",
    "        loss = torch.mean(torch.stack([x['loss'] for x in step_outputs]))\n",
    "        acc = torch.mean(torch.stack([x['acc'] for x in step_outputs]))\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sitting-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "emb_dim = 100\n",
    "output_dim = 2\n",
    "\n",
    "model = NBOW(input_dim, emb_dim, output_dim, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "identified-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = pl.callbacks.EarlyStopping(monitor='valid_loss',\n",
    "                                                     mode='min',\n",
    "                                                     patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "valued-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='valid_loss',\n",
    "                                                   mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "engaging-departure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     gpus=-1,\n",
    "                     callbacks=[early_stopping_callback,\n",
    "                                checkpoint_callback],\n",
    "                     deterministic=True,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     progress_bar_refresh_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rural-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 2.8 M \n",
      "1 | fc        | Linear    | 202   \n",
      "----------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  Train Loss: 0.689 | Train Acc: 58.89%\n",
      "  Valid Loss: 0.685 | Valid Acc: 63.93%\n",
      "Epoch:  1\n",
      "  Train Loss: 0.675 | Train Acc: 68.34%\n",
      "  Valid Loss: 0.665 | Valid Acc: 70.68%\n",
      "Epoch:  2\n",
      "  Train Loss: 0.647 | Train Acc: 73.36%\n",
      "  Valid Loss: 0.631 | Valid Acc: 74.27%\n",
      "Epoch:  3\n",
      "  Train Loss: 0.605 | Train Acc: 76.33%\n",
      "  Valid Loss: 0.589 | Valid Acc: 76.96%\n",
      "Epoch:  4\n",
      "  Train Loss: 0.562 | Train Acc: 79.28%\n",
      "  Valid Loss: 0.548 | Valid Acc: 79.05%\n",
      "Epoch:  5\n",
      "  Train Loss: 0.519 | Train Acc: 81.99%\n",
      "  Valid Loss: 0.510 | Valid Acc: 80.96%\n",
      "Epoch:  6\n",
      "  Train Loss: 0.480 | Train Acc: 84.12%\n",
      "  Valid Loss: 0.477 | Valid Acc: 82.44%\n",
      "Epoch:  7\n",
      "  Train Loss: 0.444 | Train Acc: 85.72%\n",
      "  Valid Loss: 0.449 | Valid Acc: 83.84%\n",
      "Epoch:  8\n",
      "  Train Loss: 0.412 | Train Acc: 86.88%\n",
      "  Valid Loss: 0.424 | Valid Acc: 84.95%\n",
      "Epoch:  9\n",
      "  Train Loss: 0.385 | Train Acc: 88.02%\n",
      "  Valid Loss: 0.404 | Valid Acc: 85.73%\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "radical-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.369 | Test Acc: 88.47%\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.test(test_dataloaders=test_loader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fatty-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, text_transforms, model):\n",
    "    model.eval()\n",
    "    tensor = text_transforms(sentence).unsqueeze(-1).to(model.device)\n",
    "    prediction = model(tensor)\n",
    "    probabilities = nn.functional.softmax(prediction, dim=-1)\n",
    "    pos_probability = probabilities.squeeze(0)[-1].item()\n",
    "    return pos_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "floral-bennett",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7020774768545834e-09"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'the absolute worst movie of all time.'\n",
    "\n",
    "predict(sentence, text_transforms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "recognized-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'one of the greatest films i have ever seen in my life.'\n",
    "\n",
    "predict(sentence, text_transforms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "expanded-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619628190994263"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"i thought it was going to be one of the greatest films i have ever seen in my life, \\\n",
    "but it was actually the absolute worst movie of all time.\"\n",
    "\n",
    "predict(sentence, text_transforms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "molecular-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619628190994263"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"i thought it was going to be the absolute worst movie of all time, \\\n",
    "but it was actually one of the greatest films i have ever seen in my life.\"\n",
    "\n",
    "predict(sentence, text_transforms, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
